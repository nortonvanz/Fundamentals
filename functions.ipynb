{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNjnoZIv+xyN29OGN9DcKoa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nortonvanz/Fundamentals/blob/main/functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats"
      ],
      "metadata": {
        "id": "IB7R8KkAO4Gj"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perc_diff(val_ini, val_fin):\n",
        "  # calculates the percentage difference between 2 values\n",
        "  diff = val_fin - val_ini\n",
        "  difp = (diff / val_ini) * 100\n",
        "  return round (difp, 2)"
      ],
      "metadata": {
        "id": "23IjCkBmXeSq"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query_teradata (query_from_td):\n",
        "  # connect to Teradata, run query, return result on dataframe\n",
        "  import teradatasql\n",
        "  import getpass\n",
        "\n",
        "  user = 'user'\n",
        "  passw = getpass-getpass( 'Type your pass: ')\n",
        "  host = 'host'\n",
        "\n",
        "  #this way it closes conn, avoiding errors with more than 6 conns\n",
        "  with teradatasql.connect(None, host=host, user=user, password=passw, logmech= 'LDAP') as con:\n",
        "    # SEL TOP 5 * FROM SCHEME.TABLE\n",
        "    query = f\"\"\"\n",
        "    \"\"\"+query_from_td+\"\"\"\n",
        "    \"\"\"\n",
        "    crsr = con.cursor()\n",
        "    rows = crsr.execute(query).fetchall()\n",
        "    dft = pd.DataFrame.from_records(rows, columns=[x[0] for x in crsr.description])\n",
        "    dft.columns = dft.columns.str.lower()\n",
        "\n",
        "  return dft"
      ],
      "metadata": {
        "id": "sI51kbSJdQqF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_outliers(df, features, dist_from_bounds=1.5):\n",
        "  \"\"\"\n",
        "  Remove rows containing outliers, from features on parameter\n",
        "  params:\n",
        "    df = DataFrame with dat.\n",
        "    features = list of features where outliers will be removed, pass [] in case of just one.\n",
        "    return: df without outliers\n",
        "    dist_from_bounds: standard distance to consider outlier is 1.5. The biggest, less outliers will be considered. \"\"\"\n",
        "\n",
        "  initial_shape = df.shape[0]\n",
        "\n",
        "  for feature in features:\n",
        "    Q1 - df[feature].quantile(0.25)\n",
        "    Q3 - df[feature].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - dist_from_bounds * IQR # standard = 1.5\n",
        "    upper_bound = Q3 + dist_from_bounds * IQR # standard = 1.5\n",
        "    df = df[(df[feature] >= lower_bound) & (df[feature] <= upper_bound)]\n",
        "\n",
        "  final_shape = df.shape[0]\n",
        "  rows_removed = initial_shape - final_shape\n",
        "  percent_diff = round(((Final_shape - initial_shape) / initial_shape) * 100 ,1) #Calculates dif perc\n",
        "\n",
        "  print(f'{rows_removed} were removed from original df. {initial_shape} was the original num. of rows. {final_shape} is the current num. of rows. {percent_diff}% is the difference.')\n",
        "  return df"
      ],
      "metadata": {
        "id": "vq9SjxvcajN4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cpf(df_com_cpf):\n",
        "  \"\"\"\n",
        "  Transform CPF with 15 digits to 11\n",
        "  \"\"\"\n",
        "  # Remove any non-numeric characters from the CPF column\n",
        "  df_com_cpf['cpf'] = df_com_cpf['cpf'].str,replace(r'\\D','', regex-True)\n",
        "  # Verify if CPF has 15 digits\n",
        "  if df_com_cpf['cpf'].str.len().max() != 15:\n",
        "    raise ValueError(\"The CPF must contain 15 digits\")\n",
        "  # Keep the first 9 digits and the last 2 digits\n",
        "  df_com_cpf['cpf'] = df_com_cpf['cpf'].str[:9] + df_com_cpf['cpf'].str[-2:]\n",
        "  return df_com_cpf['cpf']"
      ],
      "metadata": {
        "id": "cbKvRrcEajDu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_cpf2(df_com_cpf):\n",
        "  \"\"\"\n",
        "  Transform 11 digits CPF, filling with zeros to get 15: -> 123456789[fill 0000]12\n",
        "  \"\"\"\n",
        "  df_com_cpf['cpf'] = df_com_cpf['cpf'].apply(lambda cpf: cpf.zf111(11) if len(cpf) < 11 else cpf)\n",
        "  return df_com_cpf[ 'cpf']"
      ],
      "metadata": {
        "id": "VkbdjLREai_0"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_nps(df_com_nps_e_cpf):\n",
        "  \"\"\"\n",
        "  funcão para calcular o nps a partir das notas dadas pelos clientes ---› [NPS = % promotores - % detratores]\n",
        "  param: recebe dataframe pandas contendo campo nps e cpf (pois calcula considerando cpf único)\n",
        "    deve considerar por CPF, caso contrário seria considerada 1 nota NPS por cartão do cliente, quando ele possuir mais de 1 cartão.\n",
        "  return: nps do dataframe\n",
        "  \"\"\"\n",
        "  dfc = df_com_nps_e_cpf\n",
        "  # Calculando o número de CPFs únicos com nota >= 9 (promotores)\n",
        "  promotores_count = dfc[dfc.nps >= 9]['cpf'].nunique()\n",
        "  # Calculando o número de CPFs únicos com nota <= 6 (detratores)\n",
        "  detratores_count = dfc[dfc.nps <= 6]['cpf'].nunique()\n",
        "  # Calculando o número total de CPFs únicos\n",
        "  total_cpf_count = dfc['cpf'].nunique()\n",
        "  # Verificando se o denominador é diferente de zero\n",
        "  if total_cpf_count != 0:\n",
        "    #calcula o NPS\n",
        "    res = (promotores_count / total_cpf_count) - (detratores_count / total_cpf_count)\n",
        "  else:\n",
        "    res = 0 # Atribuindo zero como valor padrão\n",
        "  # Arredondando o resultado para uma casa decimal\n",
        "  result_percent = round(res * 100, 1)\n",
        "  return result_percent"
      ],
      "metadata": {
        "id": "XAjj-l1Zhxyp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calcula_margem_erro(df_com_nps):\n",
        "  \"\"\"\n",
        "  Entrada: df contendo coluna 'nps', sendo a Nota de NPS. A função vai obter detratores, promotores e neutros.\n",
        "  PS: não trata múltiplas respostas do mesmo CPF, como a função de calcula_nps faz.\n",
        "  \"\"\"\n",
        "  #obtem grupos nps\n",
        "  df_com_nps['nps_classe'] = 'tbd'\n",
        "  df_com_nps['nps_classe'] = df_com_nps.nps.apply(lambda x:\n",
        "                                                  'promotor' if x >= 9 else\n",
        "                                                  'detrator' if x <= 6 else 'neutro')\n",
        "  pro = df_com_nps.nps_classe.value_counts().promotor\n",
        "  neu = df_com_nps.nps_classe.value_counts().neutro\n",
        "  det = df_com_nps.nps_classe.value_counts().detrator\n",
        "  tot_res = df_com_nps.nps_classe.count()\n",
        "  #print(pro, neu, det, tot_res)\n",
        "  p = pro/tot_res\n",
        "  q = det/tot_res\n",
        "  var = (p+q-(p-q)**2)/tot_res\n",
        "  z_score = stats.norm.ppf(0.975)#índice de confiança de 95% = 0.975\n",
        "  std = np.sqrt(var)\n",
        "  margem_erro = (z_score*std*100)\n",
        "  return margem_erro"
      ],
      "metadata": {
        "id": "w2DZfWfQhxdO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def nps_por_feature_com_margem(dataframe_com_nps, feature_para_quebra):\n",
        "  \"\"\"\n",
        "  função para calcular o nps de alguma feature. Retornar também a margem de erro (com índice de confiança de 95% fixo), e o NPS inferior e superior considerando a margem.\n",
        "  entrada: recebe um DF, que deve conter no mínimo o nps e o cpf e qual a feature_para_quebra.\n",
        "  saída: dataframe contendo [feat, feat_queb_prop, feat_queb_amos, feat_queb_nps, feat_queb_nps min, feat_queb_nps_max, feat_queb_ mar_erro]\n",
        "  ex: passar feature nps_bandeira, irá retornar a quebra por bandeira.\n",
        "  Caso queira obter o resultado por alguma variação da feature, como só a variação 'classic' da feature segmento_cliente_simp, já passar o df apenas com o recorte 'classic'.\n",
        "  \"\"\"\n",
        "  dfn = dataframe_com_nps\n",
        "  feat = feature_para_quebra #ex bandeira\n",
        "  #cria base para dataset de saída, só com cols: nps, feature\n",
        "  dfn = dfn[[' cpf', 'nps', feat]].copy()\n",
        "  #remove eventuais rows cuja feature possui NAs, e printa:\n",
        "  rows_na_rem = len(dfn.loc[dfn[feat].isna() == True, feat])\n",
        "  #mantem so as nao nulas\n",
        "  dfn = dfn. loc[dfn[feat].isna() == False].copy()\n",
        "  #cria a feature de proporção:\n",
        "  feat_queb_prop = feat+'_prop'# necessário nome da feature adiante\n",
        "  dfn[feat_queb_prop] = 'tbd'\n",
        "  #cria a feature do número de amostras:\n",
        "  feat_queb_amos = feat+'_amos' #necessário nome da feature adiante\n",
        "  dfn [feat_queb_amos] = 'tbd'\n",
        "  #cria a feature da margem de erro:\n",
        "  feat_queb_mar_erro = feat+'_mrg_erro' #necessário nome da feature adiante\n",
        "  dfn[feat_queb_mar_erro] = 'tbd'\n",
        "  # Crie o dicionário contendo a feature e sua proporção %\n",
        "  dict_propor = round(dfn[feat].value_counts(normalize=True) * 100 ,1)\n",
        "  var_dict = dict_propor.to_dict()\n",
        "  #pra cada valor do dicionário var dict, se a chave do dicionário existir como feature do DF, atribui. Senão retorna x.\n",
        "  dfn[feat_queb_prop] = dfn[feat].apply(lambda x:var_dict.get(x, x))\n",
        "\n",
        "    #Cria outro dicionário, contendo a feature e o n absoluto de amostras de cada variação\n",
        "  dict_amos = dfn[feat].value_counts()\n",
        "  var_dict_amos = dict_amos.to_dict()\n",
        "  #pra cada valor do dicionário feat_queb_amos, se a chave do dicionário existir como feature do DF, atribui.\n",
        "  #Senão retorna x.\n",
        "  dfn[feat_queb_amos] = dfn[feat]-apply(lambda x:var_dict_amos.get(x, x))\n",
        "  #copiar o dicionário que já tem as variações possíveis da feature, para usar de base no laço, que vai consultar o NPS\n",
        "  var_dict_nps = var_dict.copy()\n",
        "\n",
        "  # Definir todos os valores como nulos (por segurança)\n",
        "  for chave in var_dict_nps:\n",
        "    var_dict_nps[chave] = None\n",
        "\n",
        "  #percorrer o dicionário da feature, onde cada iteração é uma variação da feature recebida por parâmetro:\n",
        "  for chave in var_dict_nps:\n",
        "    #crio dataframe para cada chave do dicionário\n",
        "    df_tmp_nps = dfn.loc[dfn[feat] == chave].copy()\n",
        "    #populo var_dict_nps, com o NPS de cada chave (variação da feature)\n",
        "    var_dict_nps[chave] = calcula_nps(df_tmp_nps)\n",
        "    #cria a feature do df retorno, que vai receber o NPS\n",
        "    feat_queb_nps = feat+'_nps'\n",
        "    dfn[feat_queb_nps] = 'tbd'\n",
        "    #pra cada valor do dicionário var_dict_nps (cada NPS), atribuo ao dataframe, caso exista esta chave. Else 0.\n",
        "    dfn[feat_queb_nps] = dfn[feat].apply(lambda x: var_dict_nps.get(x, 0))\n",
        "    #obtém as margens de erro, passando o dataset de cada variação da feature:\n",
        "    margem_iteracao = calcula_margem_erro(df_tmp_nps)\n",
        "    #quando a variação da feature for igual a chave, atribui a margem de erro:\n",
        "    dfn.loc[dfn[feat] == chave, feat_queb_mar_erro] = margem_iteracao\n",
        "\n",
        "  #remove o CPF de dfn, só necessário para a função calcula_nps\n",
        "  dfn.drop(['cpf', 'nps'], axis=1, inplace=True)\n",
        "  #dropa duplicados, mantendo apenas 1 linha por variação da feature passada:\n",
        "  dfn = dfn.drop_duplicates().sort_values(by=feat).reset_index(drop=True)\n",
        "  #converte vars para float/int, para já ficar pronto para plotar\n",
        "  dfn[feat_queb_prop] = dfn[feat_queb_prop].astype(float)\n",
        "  dfn[feat_queb_nps] = dfn[feat_queb_nps].astype(float)\n",
        "  dfn[feat_queb_amos] = dfn[feat_queb_amos].astype(int)\n",
        "  dfn[feat_queb_mar_erro] = dfn[feat_queb_mar_erro].astype(float)\n",
        "\n",
        "  #cria a feature do NPS minimo e máximo, considerando a margem de erro:\n",
        "  feat_queb_nps_min = feat+'_nps_min'\n",
        "  dfn[feat_queb_nps_min] = 'tbd'\n",
        "  feat_queb_nps_max = feat+'_nps_max'\n",
        "  dfn[feat_queb_nps_max] = 'tbd'\n",
        "  #cria o NPS mínimo e máximo, considerando a margem de erro:\n",
        "  dfn[feat_queb_nps_min] = (dfn[feat_queb_nps] - dfn[feat_queb_mar_erro])\n",
        "  dfn[feat_queb_nps_max] = (dfn[feat_queb_nps] + dfn[feat_queb_mar_erro])\n",
        "  #converte vars para float/int, para já ficar pronto para plotar\n",
        "  dfn[feat_queb_nps_min] = dfn[feat_queb_nps_min].astype(float)\n",
        "  dfn[feat_queb_nps_max] = dfn[feat_queb_nps_max].astype(float)\n",
        "  #Reordenar cols\n",
        "  cols_neworder = [feat, feat_queb_prop, feat_queb_amos, feat_queb_nps, feat_queb_ps_min, feat_queb_nps_max, feat_queb_mar_erro]\n",
        "  dfn = dfn.reindex(columns = cols_neworder)\n",
        "  #retorna o dataframe, pronto para o plot de NPS x Variações da feature:\n",
        "  return dfn"
      ],
      "metadata": {
        "id": "khU5CwVRjdsh"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NmpcWXD1kIof"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}